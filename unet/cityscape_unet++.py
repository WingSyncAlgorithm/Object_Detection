# -*- coding: utf-8 -*-
"""cityscape_Unet++

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18DkZDL3CrVb9Vp5HlZf6jz0FI8gKaAR_
"""

# setup
import os
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

from tqdm import tqdm
import gc

device = "cuda:0" if torch.cuda.is_available() else "cpu"
device = torch.device(device)
print(device)

data_dir = "C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\cityscapes_data"
train_dir = os.path.join(data_dir, "train")
val_dir = os.path.join(data_dir, "val")
train_fns = os.listdir(train_dir)
val_fns = os.listdir(val_dir)
print(len(train_fns), len(val_fns))

"""
from google.colab import drive
drive.mount('/content/drive')
"""

# analyze data
sample_image_fp = os.path.join(train_dir, train_fns[0])
sample_image = Image.open(sample_image_fp).convert("RGB")  #<PIL.Image.Image image mode=RGB size=512x256 at 0x79C6AC465E70>
plt.imshow(sample_image)

print(sample_image_fp)

def split_image(image):
    image = np.array(image)
    cityscape, label = image[:, :256, :], image[:, 256:, :]  #[y,x,rgb_channel]
    return cityscape, label

sample_image = np.array(sample_image)
print(sample_image.shape)
cityscape, label = split_image(sample_image)
print(cityscape.min(), cityscape.max(), label.min(), label.max())
cityscape, label = Image.fromarray(cityscape), Image.fromarray(label)
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(cityscape)
axes[1].imshow(label)

#define label
"""
color_set = set()
for train_fn in tqdm(train_fns[:10]):
    train_fp = os.path.join(train_dir, train_fn)
    image = np.array(Image.open(train_fp))
    cityscape, label = split_image(sample_image)
    label = label.reshape(-1, 3)
    local_color_set = set([tuple(c) for c in list(label)])
    color_set.update(local_color_set)
color_array = np.array(list(color_set))
"""

num_items = 1000
color_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)
print(color_array.shape)
print(color_array[:5, :])
num_classes = 34
label_model = KMeans(n_clusters=num_classes)
label_model.fit(color_array)
label_model.predict(color_array[:5, :])

cityscape, label = split_image(sample_image)
label_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(cityscape)
axes[1].imshow(label)
axes[2].imshow(label_class)

# define dataset
class CityscapeDataset(Dataset):

    def __init__(self, image_dir, label_model):
        self.image_dir = image_dir
        self.image_fns = os.listdir(image_dir)
        self.label_model = label_model

    def __len__(self):
        return len(self.image_fns)

    def __getitem__(self, index):
        image_fn = self.image_fns[index]  #file name
        image_fp = os.path.join(self.image_dir, image_fn) #file path
        image = Image.open(image_fp).convert('RGB')
        image = np.array(image)
        cityscape, label = self.split_image(image)
        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)
        cityscape = self.transform(cityscape)
        label_class = torch.Tensor(label_class).long()
        return cityscape, label_class

    def split_image(self, image):
        image = np.array(image)
        cityscape, label = image[:, :256, :], image[:, 256:, :]
        return cityscape, label

    def transform(self, image):
        transform_ops = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
        ])
        return transform_ops(image)

dataset = CityscapeDataset(train_dir, label_model)
print(len(dataset))

cityscape, label_class = dataset[0]
print(cityscape.shape, label_class.shape)

#define model
class UNetpp(nn.Module):

    def __init__(self, num_classes):
        super(UNetpp, self).__init__()
        self.num_classes = num_classes
        self.downconv1 = self.conv_block(in_channels=3, out_channels=64)
        self.downpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.downconv2 = self.conv_block(in_channels=64, out_channels=128)
        self.downpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.downconv3 = self.conv_block(in_channels=128, out_channels=256)
        self.downpool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.downconv4 = self.conv_block(in_channels=256, out_channels=512)
        self.downpool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.hericonv4 = self.conv_block(in_channels=512, out_channels=1024)
        self.hericonv3 = self.conv_block(in_channels=256, out_channels=512)
        self.hericonv2 = self.conv_block(in_channels=128, out_channels=256)
        self.hericonv1 = self.conv_block(in_channels=64, out_channels=128)

        #up L4
        self.upconv441 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv442 = self.conv_block(in_channels=1024, out_channels=512)
        self.upconv431 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv432 = self.conv_block(in_channels=768, out_channels=256)
        self.upconv421 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv422 = self.conv_block(in_channels=512, out_channels=128)
        self.upconv411 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv412 = self.conv_block(in_channels=320, out_channels=64)

        #up L3
        self.upconv331 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv332 = self.conv_block(in_channels=512, out_channels=256)
        self.upconv321 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv322 = self.conv_block(in_channels=384, out_channels=128)
        self.upconv311 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv312 = self.conv_block(in_channels=256, out_channels=64)

        #up L2
        self.upconv221 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv222 = self.conv_block(in_channels=256, out_channels=128)
        self.upconv211 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv212 = self.conv_block(in_channels=192, out_channels=64)

        #up L1
        self.upconv111 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.upconv112 = self.conv_block(in_channels=128, out_channels=64)

        self.output = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=3, stride=1, padding=1)

    def conv_block(self, in_channels, out_channels):
        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),
                                    nn.ReLU(),
                                    nn.BatchNorm2d(num_features=out_channels),
                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),
                                    nn.ReLU(),
                                    nn.BatchNorm2d(num_features=out_channels))
        return block

    def forward(self, X):
        #down
        c1 = self.downconv1(X) # [-1, 64, 256, 256]
        d1 = self.downpool1(c1) # [-1, 64, 128, 128]
        c2 = self.downconv2(d1) # [-1, 128, 128, 128]
        d2 = self.downpool2(c2) # [-1, 128, 64, 64]
        c3 = self.downconv3(d2) # [-1, 256, 64, 64]
        d3 = self.downpool3(c3) # [-1, 256, 32, 32]
        c4 = self.downconv4(d3) # [-1, 512, 32, 32]
        d4 = self.downpool4(c4) # [-1, 512, 16, 16]

        #up L1
        #print("L1")
        m1  = self.hericonv1(d1)  # [-1,128,128,128]
        up111 = self.upconv111(m1) # [-1,64,256,256]
        #print("112")
        up112 = self.upconv112(torch.cat((c1,up111),dim=1)) #[-1,128,256,256] -> [-1,64,256,256]
        #up L2
        #print("L2")
        m2  = self.hericonv2(d2) #[-1,256,64,64]
        up221 = self.upconv221(m2) #[-1,128,128,128]
        #print("222")
        up222 = self.upconv222(torch.cat((c2,up221),dim=1)) #[-1,256,128,128] -> [-1,128,128,128]
        up211 = self.upconv211(up222) #[-1,64,256,256]
        #print(212)
        up212 = self.upconv212(torch.cat((c1,up111,up211),dim=1)) #[-1,128,256,256] -> [-1,64,256,256]
        #up L3
        #print("L3")
        m3  = self.hericonv3(d3) #[-1,512,32,32]
        up331 = self.upconv331(m3) #[-1,256,64,64]
        #print(332)
        up332 = self.upconv332(torch.cat((c3,up331),dim=1)) #[-1,512,64,64] -> [-1,256,64,64]
        up321 = self.upconv321(up332) #[-1,128,128,128]
        #print(322)
        up322 = self.upconv322(torch.cat((c2,up221,up321),dim=1)) #[-1,256,128,128] -> [-1,128,128,128]
        up311 = self.upconv311(up322) #[-1,64,256,256]
        #print(312)
        up312 = self.upconv312(torch.cat((c1,up111,up211,up311),dim=1)) #[-1,128,256,256] -> [-1,64,256,256]

        #up L4
        #print("L4")
        m4  = self.hericonv4(d4) #[-1,1024,16,16]
        #print(441)
        up441 = self.upconv441(m4) #[-1,512,32,32]
        #print(442)
        up442 = self.upconv442(torch.cat((c4,up441),dim=1)) #[-1,1024,32,32] -> [-1,512,32,32]
        up431 = self.upconv431(up442) #[-1,256,64,64]
        #print(432)
        up432 = self.upconv432(torch.cat((c3,up331,up431),dim=1)) #[-1,512,64,64] -> [-1,256,64,64]
        up421 = self.upconv421(up432) #[-1,128,128,128]
        #print(422)
        up422 = self.upconv422(torch.cat((c2,up221,up321,up421),dim=1)) #[-1,256,128,128] -> [-1,128,128,128]
        up411 = self.upconv411(up422) #[-1,64,256,256]
        #print(412)
        up412 = self.upconv412(torch.cat((c1,up111,up211,up311,up411),dim=1)) #[-1,128,256,256] -> [-1,64,256,256]
        #print("output")
        output = self.output(torch.cat((up112,up212,up312,up412),dim=1)) #[]

        return output

model = UNetpp(num_classes=num_classes)

data_loader = DataLoader(dataset, batch_size=4)
print(len(dataset), len(data_loader))

X, Y = iter(data_loader).__next__()
print(X.shape, Y.shape)

import gc
gc.collect()

Y_pred = model(X)
print(Y_pred.shape)

#Train model
batch_size = 4

epochs = 50
lr = 0.01

dataset = CityscapeDataset(train_dir, label_model)
data_loader = DataLoader(dataset, batch_size=batch_size)

model = UNetpp(num_classes=num_classes).to(device)
try:
  model.load_state_dict(torch.load("C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\model.pth"))
except:
  pass

"""
model.train()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

gc.collect()
torch.cuda.empty_cache()
step_losses = []
epoch_losses = []
best_loss=float('inf')
try:
  with open("C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_loss.txt",'r') as f:
    best_loss=float(f.readlines()[-1].strip())
except FileNotFoundError:
  pass
except ValueError:
  idx=int(-1)
  with open("C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_loss.txt",'r') as f:
    tmp=f.readlines()
  while(1):
    #print(idx,str(idx))
    if(len(tmp[int(idx)].strip().split("."))==2):
      break
    idx=idx-1
  best_loss=float(tmp[int(idx)].strip())

model.to(device)
for epoch in tqdm(range(epochs)):
    gc.collect()
    torch.cuda.empty_cache()
    with open("C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_loss.txt",'a') as f:
      f.write("\n"+str(epoch)+":\n\n")
    epoch_loss = 0
    for X, Y in tqdm(data_loader, total=len(data_loader)):
        X, Y = X.to(device), Y.to(device)
        optimizer.zero_grad()
        Y_pred = model(X)
        loss = criterion(Y_pred, Y)
        if(loss<best_loss):
          best_loss=loss
          with open("C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_loss.txt",'a') as f:
            f.write(str(best_loss.item())+"\n")
          torch.save(model.state_dict(),'C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_model.pth')
          print('\rTrain_loss: ', loss.item(),end="")
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        step_losses.append(loss.item())
    epoch_losses.append(epoch_loss/len(data_loader))

gc.collect()
torch.cuda.empty_cache()

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].plot(step_losses)
axes[1].plot(epoch_losses)

model_name = "C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\U-Net.pth"
torch.save(model.state_dict(), model_name)
"""
#check momdel predictions
model_path = "C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\U-Net.pth"
model_ = UNetpp(num_classes=num_classes).to(device)
model_.load_state_dict(torch.load(model_path))
model_.eval()

test_batch_size = 4
dataset = CityscapeDataset(val_dir, label_model)
data_loader = DataLoader(dataset, batch_size=test_batch_size)

X, Y = next(iter(data_loader))
X, Y = X.to(device), Y.to(device)
Y_pred = model_(X)
print(Y_pred.shape)
Y_pred = torch.argmax(Y_pred, dim=1)
print(Y_pred.shape)

inverse_transform = transforms.Compose([
    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))
])

fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))

for i in range(test_batch_size):

    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()
    label_class = Y[i].cpu().detach().numpy()
    label_class_predicted = Y_pred[i].cpu().detach().numpy()

    axes[i, 0].imshow(landscape)
    axes[i, 0].set_title("Landscape")
    axes[i, 1].imshow(label_class)
    axes[i, 1].set_title("Label Class")
    axes[i, 2].imshow(label_class_predicted)
    axes[i, 2].set_title("Label Class - Predicted")

"""#best"""

#check momdel predictions
model_path = "C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_model.pth"
model1_ = UNetpp(num_classes=num_classes).to(device)
model1_.load_state_dict(torch.load(model_path))
model1_.eval()

test_batch_size1 = 8
dataset1 = CityscapeDataset(val_dir, label_model)
data_loader1 = DataLoader(dataset, batch_size=test_batch_size)

X1, Y1 = next(iter(data_loader1))
X1, Y1 = X1.to(device), Y1.to(device)
Y_pred1 = model1_(X1)
print(Y_pred1.shape)
Y_pred1 = torch.argmax(Y_pred1, dim=1)
print(Y_pred1.shape)

inverse_transform = transforms.Compose([
    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))
])

fig1, axes1 = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))

for i in range(test_batch_size):

    landscape1 = inverse_transform(X1[i]).permute(1, 2, 0).cpu().detach().numpy()
    label_class1 = Y1[i].cpu().detach().numpy()
    label_class_predicted1 = Y_pred[i].cpu().detach().numpy()

    axes1[i, 0].imshow(landscape1)
    axes1[i, 0].set_title("Landscape")
    axes1[i, 1].imshow(label_class1)
    axes1[i, 1].set_title("Label Class")
    axes1[i, 2].imshow(label_class_predicted1)
    axes1[i, 2].set_title("Label Class - Predicted")
plt.savefig("C:\\Users\\jacky\\OneDrive\\文件\\david\\cityscape\\unetpp\\best_model"+".png")
plt.show(block=False)

